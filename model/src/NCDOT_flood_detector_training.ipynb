{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6ea23a",
   "metadata": {},
   "source": [
    "# Train NCDOT flood detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d03f67",
   "metadata": {},
   "source": [
    "This is a jupyter notebook for training a supervised convolutional neural network model for flood detection using traffic webcams in the NCDOT network\n",
    "\n",
    "Part of the Flood CAM ML project\n",
    "https://github.com/FloodCamML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df989c3",
   "metadata": {},
   "source": [
    "Code contributions:\n",
    "\n",
    "* Daniel Buscombe, Marda Science / USGS\n",
    "* Evan Goldstein, UNCG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ddd12d",
   "metadata": {},
   "source": [
    "Overview:\n",
    "\n",
    "> Classification targets: \n",
    "> 0 = not usable \n",
    "> 1 = not flooded\n",
    "> 2 = not sure\n",
    "> 3 = flooded\n",
    "\n",
    "\n",
    "Approach:\n",
    "\n",
    "1. Use transfer learning to initialize a mobilenet-v2 model with pre-trained weights (from imagenet) as feature extractor\n",
    "2. Add a classification head with dropout regularization\n",
    "3. Unfreeze all layers and train from scratch (both the feature extractor and the classification head)\n",
    "4. Train with 50% of the data for validation, 50% for training\n",
    "5. Use keras-tuner for hyperparameter (learning rate, dropout rate, and number of classifying densely connected neurons) optimization\n",
    "6. Use of more than 2 classes, and one-hot encoded labels, enables use of categorical cross-entropy softmax scores to be used as independent probabilities of prediction\n",
    "7. models are saved as human-readable json files, with h5 weights, plus a pb version of the model for deployment using R/shiny\n",
    "\n",
    "Use of automated hyperparameter tuning means this approach may be more adaptable to different datasets and classification targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32375ca",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.cm as cm\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns #extended functionality / style to matplotlib plots\n",
    "from random import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "#import the tf stuff\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f931062",
   "metadata": {},
   "source": [
    "Print some important things to screen. We need tf version 2.3 or greater, and a GPU for training (ideally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e8f43",
   "metadata": {},
   "source": [
    "Hard-code the path to the data like I have done. Each image file name is modified to contain the class string ('Water', 'No_water', 'Not_sure', and 'Not_usable'), and placed into a subfolder of the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19711768",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../../../../HX_Ted_2020_NCTC/All_Photos/Recoded'\n",
    "\n",
    "mode='categorical'\n",
    "# mode = 'binary'\n",
    "\n",
    "if mode == 'binary':\n",
    "    #put names in alphabetical order\n",
    "    CLASSES = ['No_water','Water']\n",
    "    class_dict={'No_water':0, 'Water':1}\n",
    "    all_files = glob(root+'/Water/*.jpg')+glob(root+'/No_water/*.jpg')\n",
    "    total_files = len(all_files)\n",
    "else:\n",
    "    #put names in alphabetical order\n",
    "    CLASSES = ['Not_usable','No_water','Not_sure','Water']\n",
    "    class_dict={'Not_usable':0, 'No_water':1, 'Not_sure':2,  'Water':3}\n",
    "    all_files = glob(root+'/Water/*.jpg')+glob(root+'/No_water/*.jpg')+glob(root+'/Not_sure/*.jpg')+glob(root+'/Not_usable/*.jpg')\n",
    "    total_files = len(all_files)\n",
    "\n",
    "NCLASSES = len(CLASSES)\n",
    "\n",
    "train_files = all_files[::2]\n",
    "test_files = all_files[1::2]\n",
    "\n",
    "#randomize\n",
    "shuffle(train_files)\n",
    "shuffle(test_files)\n",
    "\n",
    "print('%i train files' % (len(train_files)))\n",
    "print('%i test files' % (len(test_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a7045",
   "metadata": {},
   "source": [
    "These are the hyperparameters you need to specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b73308",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRATCH = True # or False for no training from scratch\n",
    "height_width = 224 # size of images (must be square)\n",
    "BS = 16 #batch size\n",
    "EPOCHS = 200 #maximum number of training epochs\n",
    "PATIENCE = 25 #number of training epochs over which no improvement in validation loss, before terminating training\n",
    "VAL_SPLIT = 0.5 #proportion of dataset to use for validation (>= 0.5 is recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34a112",
   "metadata": {},
   "source": [
    "Function for building the model and testing a range of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93888782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##==============================================\n",
    "def model_builder(hp):\n",
    "\n",
    "    imshape = (height_width,height_width,3)\n",
    "\n",
    "    # #base_model, no top layer, w/ imagenet weights\n",
    "    # base_model = tf.keras.applications.MobileNetV2(input_shape = imshape,\n",
    "    #                                              include_top = False,\n",
    "    #                                              weights = None)\n",
    "\n",
    "    #base_model, no top layer, w/ imagenet weights\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape = imshape,\n",
    "                                                 include_top = False,\n",
    "                                                 weights = 'imagenet')\n",
    "\n",
    "    if SCRATCH:\n",
    "        base_model.trainable = True\n",
    "    else:\n",
    "        base_model.trainable = False\n",
    "    # base_model.summary()\n",
    "\n",
    "    # add a new classifcation head\n",
    "    final_layer = base_model.get_layer('out_relu')\n",
    "    #print('shape of last layer is ', final_layer.output_shape)\n",
    "    final_base_output = final_layer.output\n",
    "\n",
    "    #add the last layer\n",
    "    # GL.Av Pool\n",
    "    x = layers.GlobalAveragePooling2D()(final_base_output)\n",
    "    \n",
    "    # Add a fully connected layer with hidden units and ReLU activation\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=128, max_value=512, step=32)\n",
    "\n",
    "    x = layers.Dense(hp_units, activation='relu')(x)\n",
    "\n",
    "    dropout_rate = hp.Choice('dropout_rate', values=[.4,.5,.6])\n",
    "\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Add a final sigmoid layer for classification\n",
    "    if mode=='binary':\n",
    "        x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    else:\n",
    "        x = layers.Dense(NCLASSES, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(base_model.input, x)\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-4, 1e-5, 1e-6])\n",
    "\n",
    "    if mode is 'binary':\n",
    "        acc_metric = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "\n",
    "        #build the model\n",
    "        model.compile(loss = 'binary_crossentropy',\n",
    "                      optimizer = tf.keras.optimizers.Adam(lr = hp_learning_rate),\n",
    "                      metrics = acc_metric)\n",
    "    else:\n",
    "\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=False,\n",
    "            name='categorical_crossentropy')\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(lr= hp_learning_rate), \n",
    "                      loss=loss_fn,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf641b",
   "metadata": {},
   "source": [
    "Function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##==============================================\n",
    "def model_trainer(model,aug=False):\n",
    "\n",
    "    # set checkpoint file\n",
    "    model_checkpoint = ModelCheckpoint(weights_file, monitor='val_loss',\n",
    "                                    verbose=0, save_best_only=True, mode='min',\n",
    "                                    save_weights_only = True)\n",
    "\n",
    "    callbacks = [earlystop,model_checkpoint]\n",
    "\n",
    "    if mode is 'binary':\n",
    "\n",
    "        if aug:\n",
    "            history = model.fit(img_generator, epochs=EPOCHS, batch_size=BS, callbacks=callbacks, validation_data=val_generator)\n",
    "\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                batch_size=BS,\n",
    "                epochs=EPOCHS,\n",
    "                # We pass some validation for\n",
    "                # monitoring validation loss and metrics\n",
    "                # at the end of each epoch\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks =[callbacks])\n",
    "    else:\n",
    "\n",
    "        if aug:\n",
    "            history = model.fit(img_generator, epochs=EPOCHS, batch_size=BS, callbacks=callbacks, validation_data=val_generator)\n",
    "\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                x_train,\n",
    "                tf.one_hot(y_train,NCLASSES),\n",
    "                batch_size=BS,\n",
    "                epochs=EPOCHS,\n",
    "                # We pass some validation for\n",
    "                # monitoring validation loss and metrics\n",
    "                # at the end of each epoch\n",
    "                validation_data=(x_test, tf.one_hot(y_test,NCLASSES)),\n",
    "                callbacks =[callbacks])\n",
    "\n",
    "    #look at the metrics from training\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend(loc=0)\n",
    "    # plt.show()\n",
    "    plt.savefig(weights_file.replace('.h5','_acc.png'), dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'r--', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b--', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend(loc=0)\n",
    "    # plt.show()\n",
    "    plt.savefig(weights_file.replace('.h5','_loss.png'), dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "    val_acc_per_epoch = history.history['val_accuracy']\n",
    "    best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "    print('Best epoch: %d' % (best_epoch,))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a16355",
   "metadata": {},
   "source": [
    "Function for \n",
    "\n",
    "1. reading all train and test images into memory, and standardizing them for model training\n",
    "2. stripping names from filenames and looking up class integer code from class dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##==============================================\n",
    "def get_data(train_files,test_files,height_width):\n",
    "    '''\n",
    "    read and resize and standardize imager from files to numpy arrays\n",
    "    '''\n",
    "    x_train = np.zeros((len(train_files),height_width,height_width,3))\n",
    "    for counter,f in enumerate(train_files):\n",
    "        im = resize(imread(f), (height_width,height_width))\n",
    "        x_train[counter]=standardize(im)\n",
    "    x_train = x_train.astype(\"float32\") #/ 255.0\n",
    "\n",
    "    x_test = np.zeros((len(test_files),height_width,height_width,3))\n",
    "    for counter,f in enumerate(test_files):\n",
    "        im = resize(imread(f), (height_width,height_width))\n",
    "        x_test[counter]=standardize(im)\n",
    "\n",
    "    x_test = x_test.astype(\"float32\") #/ 255.0\n",
    "\n",
    "    y_train = []\n",
    "    for f in train_files:\n",
    "        y_train.append(class_dict[f.split(os.sep)[-1].split('_X_')[0]])\n",
    "\n",
    "    y_train = np.expand_dims(y_train,-1).astype('uint8')\n",
    "    y_train = np.squeeze(y_train)\n",
    "\n",
    "    y_test = []\n",
    "    for f in test_files:\n",
    "        y_test.append(class_dict[f.split(os.sep)[-1].split('_X_')[0]])\n",
    "    y_test = np.expand_dims(y_test,-1).astype('uint8')\n",
    "\n",
    "    y_test = np.squeeze(y_test)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108f83f",
   "metadata": {},
   "source": [
    "Functions for image standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #IMG UTILS\n",
    "##==============================================\n",
    "def standardize(img):\n",
    "    '''\n",
    "    standardization using adjusted standard deviation,\n",
    "    then rescales an input dat between mn and mx\n",
    "    '''\n",
    "    N = np.shape(img)[0] * np.shape(img)[1]\n",
    "    s = np.maximum(np.std(img), 1.0/np.sqrt(N))\n",
    "    m = np.mean(img)\n",
    "    img = (img - m) / s\n",
    "    img = rescale(img, 0, 1)\n",
    "    del m, s, N\n",
    "    if np.ndim(img)!=3:\n",
    "        img = np.dstack((img,img,img))\n",
    "\n",
    "    return img\n",
    "\n",
    "##==============================================\n",
    "def rescale(dat,mn,mx):\n",
    "    '''\n",
    "    rescales an input dat between mn and mx\n",
    "    '''\n",
    "    m = min(dat.flatten())\n",
    "    M = max(dat.flatten())\n",
    "    return (mx-mn)*(dat-m)/(M-m)+mn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec3aa0",
   "metadata": {},
   "source": [
    "Functions to compute and apply gradCAM feature importances mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://keras.io/examples/vision/grad_cam/\n",
    "##==============================================\n",
    "# #GRADCAM\n",
    "##==============================================\n",
    "def make_gradcam_heatmap(\n",
    "    img_array, model, last_conv_layer_name, classifier_layer_names\n",
    "):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    last_conv_layer_model = Model(model.inputs, last_conv_layer.output)\n",
    "\n",
    "    # Second, we create a model that maps the activations of the last conv\n",
    "    # layer to the final class predictions\n",
    "    classifier_input = tf.keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        x = model.get_layer(layer_name)(x)\n",
    "    classifier_model = tf.keras.Model(classifier_input, x)\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute activations of the last conv layer and make the tape watch it\n",
    "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        # Compute class predictions\n",
    "        preds = classifier_model(last_conv_layer_output)\n",
    "        top_pred_index = tf.argmax(preds[0])\n",
    "        top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "    # This is the gradient of the top predicted class with regard to\n",
    "    # the output feature map of the last conv layer\n",
    "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
    "    #grads = tape.gradient(bottom_class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    return heatmap\n",
    "\n",
    "##==============================================\n",
    "def do_gradcam_viz(img, model, outfile):\n",
    "    #define last conv layers\n",
    "    last_conv_layer_name = \"out_relu\"\n",
    "    classifier_layer_names = [\n",
    "        \"global_average_pooling2d\",\n",
    "        \"dense\",\n",
    "        \"dropout\",\n",
    "        \"dense_1\",\n",
    "    ]\n",
    "    \n",
    "    alpha=0.4\n",
    "\n",
    "    img_array = np.expand_dims(img,axis=0)\n",
    "\n",
    "    #from https://keras.io/examples/vision/grad_cam/\n",
    "    # Make the heatmap\n",
    "    try:\n",
    "        heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names)\n",
    "    except:\n",
    "\n",
    "        classifier_layer_names = [\n",
    "            \"global_average_pooling2d_1\",\n",
    "            \"dense_2\",\n",
    "            \"dropout_1\",\n",
    "            \"dense_3\",\n",
    "        ]\n",
    "        heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names)\n",
    "\n",
    "    # rescale image (range 0-255)\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # use viridis for heatmap\n",
    "    vir = cm.get_cmap(\"viridis\")\n",
    "    vir_colors = vir(np.arange(256))[:, :3]\n",
    "    vir_heatmap = vir_colors[heatmap]\n",
    "\n",
    "    # make the heatmap\n",
    "    vir_heatmap = tf.keras.preprocessing.image.array_to_img(vir_heatmap)\n",
    "    vir_heatmapI = vir_heatmap.resize((Orimg.shape[1], Orimg.shape[0]))\n",
    "    vir_heatmap = tf.keras.preprocessing.image.img_to_array(vir_heatmapI)\n",
    "\n",
    "    #put heatmpa on image\n",
    "    superimposed_img = vir_heatmap * alpha + Orimg\n",
    "    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Display Image, heatmap and overlay\n",
    "    # Display heatmap\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(121)#31)\n",
    "    plt.imshow(Orimg) ; plt.title('Orig image')\n",
    "    #tf.keras.preprocessing.image.load_img(impath, target_size = imsize))\n",
    "    # plt.subplot(132)\n",
    "    # plt.imshow(vir_heatmapI); plt.title('heatmap')\n",
    "    plt.subplot(122)#33)\n",
    "    plt.imshow(superimposed_img); plt.title('Superimposed GRADCAM')\n",
    "\n",
    "    plt.savefig(outfile, dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ef860",
   "metadata": {},
   "source": [
    "Function to read an image from file and use a trained model for gradCAM estimation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_flood(f,FloodCAMML,height_width=224):\n",
    "    '''\n",
    "    given file path string, f, and height_width (default=224)\n",
    "    return 0 for no flood and 1 for flood\n",
    "    '''\n",
    "    img = resize(imread(f), (height_width,height_width))\n",
    "    Orimg = imread(f)\n",
    "    img_array = np.expand_dims(img,axis=0)\n",
    "\n",
    "    try:\n",
    "        do_gradcam_viz(img, FloodCAMML, outfile=f.replace('.jpg','_gradcam.png'))\n",
    "    except:\n",
    "        pass\n",
    "    return FloodCAMML.predict(img_array, batch_size = 1, verbose = False)\n",
    "\n",
    "##==============================================\n",
    "def get_model_load_weights(weights_file):\n",
    "    '''\n",
    "    given file path string, weights file, loads keras model and assigns weights\n",
    "    '''\n",
    "    json_file = open(weights_file.replace('.h5','.json'), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    FloodCAMML = model_from_json(loaded_model_json)\n",
    "    #print(\"Loading weights into model\")\n",
    "    # load weights into new model\n",
    "    FloodCAMML.load_weights(weights_file)\n",
    "    return FloodCAMML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7342a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a283ec33",
   "metadata": {},
   "source": [
    "# Training begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271613e",
   "metadata": {},
   "source": [
    "Get train and test datasets (read into memory - could replace with generators later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c363d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_data(train_files,test_files,height_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ede2b",
   "metadata": {},
   "source": [
    "Grab and plot a batch to visualize / check the input data are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_train[:BS]\n",
    "y = y_train[:BS]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "counter = 1\n",
    "for im, l in zip(x,y):\n",
    "    ax = plt.subplot(4, 4,counter)\n",
    "    plt.imshow(im)\n",
    "\n",
    "    l=np.int(l)\n",
    "    plt.title(CLASSES[l], fontsize=9)\n",
    "\n",
    "    plt.axis('off')\n",
    "    counter +=1\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig('results/NCDot_example_train_batch_'+mode+'.png',dpi=300,bbox_inches='tight')\n",
    "plt.close()\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8458a",
   "metadata": {},
   "source": [
    "Use a hyperband tuner to search the hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='tuner',\n",
    "                     project_name='CamML_'+datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\",\n",
    "                              mode=\"min\", patience=PATIENCE)\n",
    "\n",
    "tuner.search(x_train, \n",
    "             tf.one_hot(y_train,NCLASSES), \n",
    "             epochs=EPOCHS, \n",
    "             validation_data=(x_test, tf.one_hot(y_test,NCLASSES)), \n",
    "             callbacks=[earlystop])\n",
    "\n",
    "# models = tuner.get_best_models(num_models=2)\n",
    "\n",
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626db1a3",
   "metadata": {},
   "source": [
    "Get the optimal hyperparameters and print to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5712076",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "optimal number of units in the first densely-connected layer = {best_hps.get('units')} \\n\n",
    "optimal learning rate for the optimizer is {best_hps.get('learning_rate')}\\n\n",
    "optimal dropout rate for the optimizer is {best_hps.get('dropout_rate')}\\n\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a9f749",
   "metadata": {},
   "source": [
    "Build the best model and create a weights file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baaec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "DROPOUT = best_hps.get('dropout_rate')\n",
    "NUM_NEURONS = best_hps.get('units')\n",
    "lr = best_hps.get('learning_rate')\n",
    "\n",
    "if SCRATCH:\n",
    "    #tl=transferlarning, mv2=MobileNetV2 feature extractor\n",
    "    weights_file = 'results/scratch_mv2_bs'+str(BS)+'_drop'+str(DROPOUT)+'_nn'+str(NUM_NEURONS)+'_sz'+str(height_width)+'_lr'+str(lr)+'_val'+str(VAL_SPLIT)+'.h5'\n",
    "\n",
    "else:\n",
    "    #tl=transferlarning, mv2=MobileNetV2 feature extractor\n",
    "    weights_file = 'results/tl_mv2_bs'+str(BS)+'_drop'+str(DROPOUT)+'_nn'+str(NUM_NEURONS)+'_sz'+str(height_width)+'_lr'+str(lr)+'_val'+str(VAL_SPLIT)+'.h5'\n",
    "\n",
    "print(weights_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd365165",
   "metadata": {},
   "source": [
    "Train the model, and save to json format, and finally to pb format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_trainer(model)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(weights_file.replace('.h5','.json'), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# save as TF.js\n",
    "# tfjs.converters.save_keras_model(model, './JSmodel')\n",
    "\n",
    "model.save('Rmodel_'+weights_file.replace('.h5','')+datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37610a5f",
   "metadata": {},
   "source": [
    "Test the model on some sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84523060",
   "metadata": {},
   "outputs": [],
   "source": [
    "notsure_files = glob(root+'/Not_sure/*.jpg')\n",
    "\n",
    "for k in np.arange(BS):\n",
    "    use = np.random.randint(100)\n",
    "    f = notsure_files[use]\n",
    "    img = resize(imread(f), (height_width,height_width))\n",
    "    Orimg = imread(f)\n",
    "    outfile = 'predict'+os.sep+f.split(os.sep)[-1].replace('.jpg','_gradcam.png')\n",
    "\n",
    "    try:\n",
    "        do_gradcam_viz(img, model, outfile)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622ec5f",
   "metadata": {},
   "source": [
    "Predict on each test image and construct a confusion matrix shoing one-on-one correspondences across all 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test).squeeze()\n",
    "\n",
    "if mode=='binary':\n",
    "    y_pred = (y_pred>.5).astype(int)\n",
    "else:\n",
    "    y_pred = np.argmax(y_pred, -1)\n",
    "\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "cmat = cmat.astype('float') / cmat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cmat,\n",
    "  annot=True,\n",
    "  cmap = sns.cubehelix_palette(dark=0, light=1, as_cmap=True))\n",
    "\n",
    "tick_marks = np.arange(len(CLASSES))+.5\n",
    "plt.xticks(tick_marks, [c for c in CLASSES], rotation=90,fontsize=12)\n",
    "plt.yticks(tick_marks, [c for c in CLASSES],rotation=0, fontsize=12)\n",
    "plt.title('N = '+str(len(y_test)), fontsize=12)\n",
    "\n",
    "plt.savefig(weights_file.replace('.h5','_cm.png'), dpi=200, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907a828",
   "metadata": {},
   "source": [
    "Fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4c96f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
